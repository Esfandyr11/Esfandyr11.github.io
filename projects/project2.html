<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lightning RAG Project</title>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: "JetBrains Mono", monospace;
            margin: 0;
            background-color: #121212;
            color: #ffffff;
        }

        nav {
            background-color: #1c1c1c;
            padding: 1rem;
            text-align: left;
        }

        nav a {
            font-family: "JetBrains Mono", monospace;
            color: #ffffff;
            margin: 0 1rem;
            text-decoration: none;
            font-weight: bold;
            transition: color 0.3s;
        }

        nav a:hover {
            color: #f39c12;
        }

        .content {
            padding: 2rem;
            max-width: 1200px;
            margin: auto;
        }

        .box {
            background-color: #2e2e2e;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            padding: 2rem;
            margin: 2rem 0;
            transition: transform 0.5s, box-shadow 0.5s;
        }

        .box img {
            max-width: 100%;
            border-radius: 8px;
        }

        h1, h2, h3 {
            color: #ffffff;
        }

        p {
            color: #b0b0b0;
        }

        .equation {
            color: #b0b0b0;
            text-align: center;
            margin: 1rem 0;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html#home">Home</a>
        <a href="../index.html#projects">Projects</a>
        <a href="../index.html#publications">Publications</a>
        <a href="../hobbies.html">Hobbies</a>
        <a href="../okresume.pdf" target="_blank">CV</a>
    </nav>
    <div class="content">
        <h1>Lightning RAG Project</h1>
        <h2>Esfandyr</h2>
        
        <div class="box">
    <h1>Setup a Localized Easy OCR with a Ray Cluster</h1>
    <p>Setting up a localized Easy OCR with a Ray cluster involves deploying distributed OCR processing using the Ray framework. This setup leverages Ray Serve for scalable and distributed model serving and utilizes multithreading to accelerate text extraction speed.</p>
    
    <p>Ray Serve is a scalable model serving library that can handle model inference requests from multiple clients. By setting up Ray Serve, we ensure that our OCR system can scale horizontally and handle a high volume of requests efficiently. This is achieved by distributing the workload across multiple nodes in a Ray cluster, each responsible for a portion of the OCR tasks.</p>
    
    <p>To accelerate text extraction, we employ multithreading within each node. Ray's actor-based concurrency model allows us to utilize all available CPU cores effectively. Each OCR task is assigned to a separate thread, enabling parallel processing and significantly reducing the time required for text extraction.</p>

    <p>In real-time OCR systems, achieving low latency and high throughput is crucial. By leveraging Ray's distributed computing capabilities, we can scale our OCR system to handle large volumes of data while maintaining low latency. This setup is particularly useful in applications where real-time text extraction is required, such as automatic license plate recognition or document digitization.</p>
    
    <h2>Implementing CRNN with CTC Loss for Text Recognition</h2>
    <p>The Convolutional Recurrent Neural Network (CRNN) combined with Connectionist Temporal Classification (CTC) loss is a powerful architecture for text recognition tasks. CRNNs are designed to handle sequential data, making them well-suited for recognizing text in images.</p>

    <p>The architecture of CRNN consists of three main components:</p>
    <ul>
        <li><strong>Convolutional Layers:</strong> These layers extract visual features from the input image. They act as feature extractors, capturing important patterns in the image that are relevant to text recognition.</li>
        <li><strong>Recurrent Layers:</strong> Recurrent Neural Networks (RNNs) process the sequence of features extracted by the convolutional layers. RNNs are capable of modeling temporal dependencies, which is crucial for recognizing text as a sequence of characters.</li>
        <li><strong>Transcription Layer:</strong> This layer converts the output of the recurrent layers into a sequence of characters. The CTC loss function is used to align the predicted sequence with the ground truth text.</li>
    </ul>
    <p>CRNNs with CTC loss have been shown to achieve state-of-the-art performance on various text recognition benchmarks. The combination of convolutional and recurrent layers allows the model to capture both spatial and temporal dependencies in the input image, leading to accurate text recognition.</p>

    <img src="    https://api.wandb.ai/files/authors/images/projects/82562/102ec7a3.png" alt="CRNN Architecture" style="max-width: 100%; border-radius: 8px;">

    <p>Setting up this architecture within the Ray cluster involves distributing the training and inference tasks across multiple nodes. Each node in the cluster is responsible for processing a subset of the data, and the results are aggregated to obtain the final output. This distributed approach ensures that the OCR system can handle large-scale text recognition tasks efficiently.</p>

    <p>For example, when processing a document with multiple pages, each page can be assigned to a different node in the Ray cluster. The nodes process the pages concurrently, extracting text from each page in parallel. Once all pages are processed, the extracted text is combined to form the final output.</p>
    <img src="https://api.wandb.ai/files/authors/images/projects/82562/a2017bee.png" alt="CRNN Training Process" style="max-width: 100%; border-radius: 8px;">

    <p>This setup not only improves the speed of text extraction but also ensures that the system can scale to handle large volumes of data. By leveraging the distributed computing capabilities of Ray, we can build a robust and scalable OCR system that meets the demands of real-time text recognition applications.</p>

    <pre>
        <code>
        ## Ref: https://keras.io/examples/vision/captcha_ocr/
        
        class CTCLayer(layers.Layer):
            def __init__(self, name=None):
                super().__init__(name=name)
                self.loss_fn = keras.backend.ctc_batch_cost
        
            def call(self, y_true, y_pred):
                batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
                input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64")
                label_length = tf.cast(tf.shape(y_true)[1], dtype="int64")
        
                input_length = input_length * tf.ones(shape=(batch_len, 1), dtype="int64")
                label_length = label_length * tf.ones(shape=(batch_len, 1), dtype="int64")
        
                loss = self.loss_fn(y_true, y_pred, input_length, label_length)
                self.add_loss(loss)
        
                return y_pred
        
        def ctc_decoder(predictions):
            text_list = []
            pred_indcies = np.argmax(predictions, axis=2)
            
            for i in range(pred_indcies.shape[0]):
                ans = ""
                merged_list = [k for k,_ in groupby(pred_indcies[i])]
                
                for p in merged_list:
                    if p != len(char_list):
                        ans += char_list[int(p)]
                
                text_list.append(ans)
                
            return text_list
        
        figures_list = []
        
        class PlotPredictions(tf.keras.callbacks.Callback):
            def __init__(self, frequency=1):
                self.frequency = frequency
                super(PlotPredictions, self).__init__()
        
                batch = validation_dataset.take(1)
                self.batch_images = list(batch.as_numpy_iterator())[0]["image"]
                self.batch_labels = list(batch.as_numpy_iterator())[0]["label"]
        
            def plot_predictions(self, epoch):
                prediction_model = keras.models.Model(
                    self.model.get_layer(name="image").input, 
                    self.model.get_layer(name="dense").output
                )
                
                preds = prediction_model.predict(self.batch_images)
                pred_texts = ctc_decoder(preds)
        
                orig_texts = []
                for label in self.batch_labels:
                    orig_texts.append("".join([char_list[int(char_ind)] for char_ind in label if not(char_ind == len(char_list))]))
        
                fig , ax = plt.subplots(4, 4, figsize=(15, 5))
                fig.suptitle('Epoch: '+str(epoch), weight='bold', size=14)
        
                for i in range(16):
                    img = (self.batch_images[i, :, :, 0] * 255).astype(np.uint8)
                    title = f"Prediction: {pred_texts[i]}"
                    ax[i // 4, i % 4].imshow(img, cmap="gray")
                    ax[i // 4, i % 4].set_title(title)
                    ax[i // 4, i % 4].axis("off")
                
                plt.show()
                figures_list.append(fig)
        
            def on_epoch_end(self, epoch, logs=None):
                if epoch % self.frequency == 0:
                    self.plot_predictions(epoch)
        
        def train(epochs):
            inputs = Input(shape=(32, 128, 1), name="image")
            labels = layers.Input(name="label", shape=(None,), dtype="float32")
        
            conv_1 = Conv2D(32, (3,3), activation = "selu", padding='same')(inputs)
            pool_1 = MaxPool2D(pool_size=(2, 2))(conv_1)
            
            conv_2 = Conv2D(64, (3,3), activation = "selu", padding='same')(pool_1)
            pool_2 = MaxPool2D(pool_size=(2, 2))(conv_2)
        
            conv_3 = Conv2D(128, (3,3), activation = "selu", padding='same')(pool_2)
            conv_4 = Conv2D(128, (3,3), activation = "selu", padding='same')(conv_3)
            pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)
            
            conv_5 = Conv2D(256, (3,3), activation = "selu", padding='same')(pool_4)
            batch_norm_5 = BatchNormalization()(conv_5)
            
            conv_6 = Conv2D(256, (3,3), activation = "selu", padding='same')(batch_norm_5)
            batch_norm_6 = BatchNormalization()(conv_6)
            pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)
            
            conv_7 = Conv2D(64, (2,2), activation = "selu")(pool_6)
            squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)
            
            blstm_1 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(squeezed)
            blstm_2 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(blstm_1)
            softmax_output = Dense(len(char_list) + 1, activation = 'softmax', name="dense")(blstm_2)
        
            output = CTCLayer(name="ctc_loss")(labels, softmax_output)
        
            optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, clipnorm=1.0)
            model = Model(inputs=[inputs, labels], outputs=output)
            model.compile(optimizer = optimizer)
        
            print(model.summary())
            file_path = "C_LSTM_best.hdf5"
            
            checkpoint = ModelCheckpoint(filepath=file_path, 
                                        monitor='val_loss', 
                                        verbose=1, 
                                        save_best_only=True, 
                                        mode='min')
        
            callbacks_list = [checkpoint, 
                              WandbCallback(monitor="val_loss", 
                                            mode="min", 
                                            log_weights=True),
                              PlotPredictions(frequency=1),
                              EarlyStopping(patience=3, verbose=1)]
        
            history = model.fit(train_dataset, 
                                epochs = epochs,
                                validation_data=validation_dataset,
                                verbose = 1,
                                callbacks = callbacks_list,
                                shuffle=True)
            
            return model
        </code>
        </pre>
        </div>

        
        <div class="box">
            <h3>Utilizing Multithreading with Ray to Accelerate Text Extraction</h3>
            <p>To accelerate text extraction, we leverage Ray's parallel processing capabilities. By distributing the OCR workload across multiple CPU cores, we can significantly enhance performance. This involves initializing Ray, creating multiple actor instances of an OCR worker class, and distributing the text extraction tasks among these actors. Each OCR worker uses the EasyOCR library to process images in parallel, thus speeding up the overall text extraction process.</p>
        </div>
        
        <div class="box">
            <h3>Setting Up Embedding Logic on Another Node in the Ray Cluster</h3>
            <p>The embedding logic is implemented using nomic-1.5v, fast embed, and Qdrant vector DB. First, we set up the embedding model with nomic-1.5v, a powerful embedding library. The model is initialized and used to embed texts, transforming them into high-dimensional vectors. Fast embed is integrated to ensure efficient text embeddings, providing a streamlined approach to generate embeddings for large text datasets.</p>
        </div>
        
        <div class="box">
            <h3>Creating a Customized Indexing Layer</h3>
            <p>To manage and query the high-dimensional embeddings, we implement a customized indexing layer using a hybrid approach that combines HNSW-FINGER and sparse hybrid base index with IVPFQ. HNSW-FINGER utilizes hierarchical navigable small world graphs for efficient similarity search, while IVPFQ employs inverted file and product quantization techniques. These methods enhance the indexing and retrieval performance, allowing for fast and accurate nearest neighbor searches.</p>
        </div>
        
        <div class="box">
            <h3>Retrieval Mechanism</h3>
            <p>The retrieval mechanism involves utilizing Qdrant and ChatWindowSummaryBuffer for efficient vector search and continuous chat summarization. Qdrant, a high-performance vector database, is employed to store and query the embeddings. The search operation involves querying the database with an embedding vector and retrieving the most similar vectors based on a specified distance metric.</p>
            <p>ChatWindowSummaryBuffer is used to manage long-term conversation memory, enabling continuous summarization of chat windows. This component helps in maintaining context and generating concise summaries of ongoing conversations, enhancing the user experience.</p>
        </div>
        
        <div class="box">
            <h3>AWS Cluster for File Ingestion and Embedding</h3>
            <p>For large-scale file ingestion and embedding, we set up an AWS cluster. The cluster is configured to ingest 1000 files within 1 minute and embed them within 3-4 minutes. This involves deploying a Ray cluster on AWS, specifying the number of worker nodes, and initializing Ray Serve on the cluster. The custom GROQ LLM server is integrated to handle the language model computations, ensuring efficient processing of large volumes of text data.</p>
        </div>
    </div>
</body>
</html>
