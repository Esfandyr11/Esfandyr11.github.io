<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lightning RAG Project</title>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: "JetBrains Mono", monospace;
            margin: 0;
            background-color: #121212;
            color: #ffffff;
        }

        nav {
            background-color: #1c1c1c;
            padding: 1rem;
            text-align: left;
        }

        nav a {
            font-family: "JetBrains Mono", monospace;
            color: #ffffff;
            margin: 0 1rem;
            text-decoration: none;
            font-weight: bold;
            transition: color 0.3s;
        }

        nav a:hover {
            color: #f39c12;
        }

        .content {
            padding: 2rem;
            max-width: 1200px;
            margin: auto;
        }

        .box {
            background-color: #2e2e2e;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            padding: 2rem;
            margin: 2rem 0;
            transition: transform 0.5s, box-shadow 0.5s;
        }

        .box img {
            max-width: 100%;
            border-radius: 8px;
        }

        h1, h2, h3 {
            color: #ffffff;
        }

        p {
            color: #b0b0b0;
        }

        .equation {
            color: #b0b0b0;
            text-align: center;
            margin: 1rem 0;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html#home">Home</a>
        <a href="../index.html#projects">Projects</a>
        <a href="../index.html#publications">Publications</a>
        <a href="../hobbies.html">Hobbies</a>
        <a href="../okresume.pdf" target="_blank">CV</a>
    </nav>
    <div class="content">
        <h1>Lightning RAG Project</h1>
        <h2>Esfandyr</h2>
        
        <div class="box">
    <h1>Setup a Localized Easy OCR with a Ray Cluster</h1>
    <p>Setting up a localized Easy OCR with a Ray cluster involves deploying distributed OCR processing using the Ray framework. This setup leverages Ray Serve for scalable and distributed model serving and utilizes multithreading to accelerate text extraction speed.</p>
    
    <p>Ray Serve is a scalable model serving library that can handle model inference requests from multiple clients. By setting up Ray Serve, we ensure that our OCR system can scale horizontally and handle a high volume of requests efficiently. This is achieved by distributing the workload across multiple nodes in a Ray cluster, each responsible for a portion of the OCR tasks.</p>
    
    <p>To accelerate text extraction, we employ multithreading within each node. Ray's actor-based concurrency model allows us to utilize all available CPU cores effectively. Each OCR task is assigned to a separate thread, enabling parallel processing and significantly reducing the time required for text extraction.</p>

    <p>In real-time OCR systems, achieving low latency and high throughput is crucial. By leveraging Ray's distributed computing capabilities, we can scale our OCR system to handle large volumes of data while maintaining low latency. This setup is particularly useful in applications where real-time text extraction is required, such as automatic license plate recognition or document digitization.</p>
    
    <h2>Implementing CRNN with CTC Loss for Text Recognition</h2>
    <p>The Convolutional Recurrent Neural Network (CRNN) combined with Connectionist Temporal Classification (CTC) loss is a powerful architecture for text recognition tasks. CRNNs are designed to handle sequential data, making them well-suited for recognizing text in images.</p>

    <p>The architecture of CRNN consists of three main components:</p>
    <ul>
        <li><strong>Convolutional Layers:</strong> These layers extract visual features from the input image. They act as feature extractors, capturing important patterns in the image that are relevant to text recognition.</li>
        <li><strong>Recurrent Layers:</strong> Recurrent Neural Networks (RNNs) process the sequence of features extracted by the convolutional layers. RNNs are capable of modeling temporal dependencies, which is crucial for recognizing text as a sequence of characters.</li>
        <li><strong>Transcription Layer:</strong> This layer converts the output of the recurrent layers into a sequence of characters. The CTC loss function is used to align the predicted sequence with the ground truth text.</li>
    </ul>
    <p>CRNNs with CTC loss have been shown to achieve state-of-the-art performance on various text recognition benchmarks. The combination of convolutional and recurrent layers allows the model to capture both spatial and temporal dependencies in the input image, leading to accurate text recognition.</p>

    <img src="    https://api.wandb.ai/files/authors/images/projects/82562/102ec7a3.png" alt="CRNN Architecture" style="max-width: 100%; border-radius: 8px;">

    <p>Setting up this architecture within the Ray cluster involves distributing the training and inference tasks across multiple nodes. Each node in the cluster is responsible for processing a subset of the data, and the results are aggregated to obtain the final output. This distributed approach ensures that the OCR system can handle large-scale text recognition tasks efficiently.</p>

    <p>For example, when processing a document with multiple pages, each page can be assigned to a different node in the Ray cluster. The nodes process the pages concurrently, extracting text from each page in parallel. Once all pages are processed, the extracted text is combined to form the final output.</p>
    <img src="https://api.wandb.ai/files/authors/images/projects/82562/a2017bee.png" alt="CRNN Training Process" style="max-width: 100%; border-radius: 8px;">

    <p>This setup not only improves the speed of text extraction but also ensures that the system can scale to handle large volumes of data. By leveraging the distributed computing capabilities of Ray, we can build a robust and scalable OCR system that meets the demands of real-time text recognition applications.</p>

    <pre>
        <code>
        ## Ref: https://keras.io/examples/vision/captcha_ocr/
        
        class CTCLayer(layers.Layer):
            def __init__(self, name=None):
                super().__init__(name=name)
                self.loss_fn = keras.backend.ctc_batch_cost
        
            def call(self, y_true, y_pred):
                batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
                input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64")
                label_length = tf.cast(tf.shape(y_true)[1], dtype="int64")
        
                input_length = input_length * tf.ones(shape=(batch_len, 1), dtype="int64")
                label_length = label_length * tf.ones(shape=(batch_len, 1), dtype="int64")
        
                loss = self.loss_fn(y_true, y_pred, input_length, label_length)
                self.add_loss(loss)
        
                return y_pred
        
        def ctc_decoder(predictions):
            text_list = []
            pred_indcies = np.argmax(predictions, axis=2)
            
            for i in range(pred_indcies.shape[0]):
                ans = ""
                merged_list = [k for k,_ in groupby(pred_indcies[i])]
                
                for p in merged_list:
                    if p != len(char_list):
                        ans += char_list[int(p)]
                
                text_list.append(ans)
                
            return text_list
        
        figures_list = []
        
        class PlotPredictions(tf.keras.callbacks.Callback):
            def __init__(self, frequency=1):
                self.frequency = frequency
                super(PlotPredictions, self).__init__()
        
                batch = validation_dataset.take(1)
                self.batch_images = list(batch.as_numpy_iterator())[0]["image"]
                self.batch_labels = list(batch.as_numpy_iterator())[0]["label"]
        
            def plot_predictions(self, epoch):
                prediction_model = keras.models.Model(
                    self.model.get_layer(name="image").input, 
                    self.model.get_layer(name="dense").output
                )
                
                preds = prediction_model.predict(self.batch_images)
                pred_texts = ctc_decoder(preds)
        
                orig_texts = []
                for label in self.batch_labels:
                    orig_texts.append("".join([char_list[int(char_ind)] for char_ind in label if not(char_ind == len(char_list))]))
        
                fig , ax = plt.subplots(4, 4, figsize=(15, 5))
                fig.suptitle('Epoch: '+str(epoch), weight='bold', size=14)
        
                for i in range(16):
                    img = (self.batch_images[i, :, :, 0] * 255).astype(np.uint8)
                    title = f"Prediction: {pred_texts[i]}"
                    ax[i // 4, i % 4].imshow(img, cmap="gray")
                    ax[i // 4, i % 4].set_title(title)
                    ax[i // 4, i % 4].axis("off")
                
                plt.show()
                figures_list.append(fig)
        
            def on_epoch_end(self, epoch, logs=None):
                if epoch % self.frequency == 0:
                    self.plot_predictions(epoch)
        
        def train(epochs):
            inputs = Input(shape=(32, 128, 1), name="image")
            labels = layers.Input(name="label", shape=(None,), dtype="float32")
        
            conv_1 = Conv2D(32, (3,3), activation = "selu", padding='same')(inputs)
            pool_1 = MaxPool2D(pool_size=(2, 2))(conv_1)
            
            conv_2 = Conv2D(64, (3,3), activation = "selu", padding='same')(pool_1)
            pool_2 = MaxPool2D(pool_size=(2, 2))(conv_2)
        
            conv_3 = Conv2D(128, (3,3), activation = "selu", padding='same')(pool_2)
            conv_4 = Conv2D(128, (3,3), activation = "selu", padding='same')(conv_3)
            pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)
            
            conv_5 = Conv2D(256, (3,3), activation = "selu", padding='same')(pool_4)
            batch_norm_5 = BatchNormalization()(conv_5)
            
            conv_6 = Conv2D(256, (3,3), activation = "selu", padding='same')(batch_norm_5)
            batch_norm_6 = BatchNormalization()(conv_6)
            pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)
            
            conv_7 = Conv2D(64, (2,2), activation = "selu")(pool_6)
            squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)
            
            blstm_1 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(squeezed)
            blstm_2 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(blstm_1)
            softmax_output = Dense(len(char_list) + 1, activation = 'softmax', name="dense")(blstm_2)
        
            output = CTCLayer(name="ctc_loss")(labels, softmax_output)
        
            optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, clipnorm=1.0)
            model = Model(inputs=[inputs, labels], outputs=output)
            model.compile(optimizer = optimizer)
        
            print(model.summary())
            file_path = "C_LSTM_best.hdf5"
            
            checkpoint = ModelCheckpoint(filepath=file_path, 
                                        monitor='val_loss', 
                                        verbose=1, 
                                        save_best_only=True, 
                                        mode='min')
        
            callbacks_list = [checkpoint, 
                              WandbCallback(monitor="val_loss", 
                                            mode="min", 
                                            log_weights=True),
                              PlotPredictions(frequency=1),
                              EarlyStopping(patience=3, verbose=1)]
        
            history = model.fit(train_dataset, 
                                epochs = epochs,
                                validation_data=validation_dataset,
                                verbose = 1,
                                callbacks = callbacks_list,
                                shuffle=True)
            
            return model
        </code>
        </pre>
        </div>

        
        <div class="box">
            <h1>Utilizing Multithreading with Ray to Accelerate Text Extraction</h1>
            <p>Setting up Ray clusters for OCR tasks involves configuring a distributed computing environment that can efficiently handle large-scale OCR workloads. Ray provides an easy-to-use framework for building and managing clusters, allowing us to leverage distributed computing for our OCR tasks.</p>
        
            <p>Ray clusters consist of a head node and multiple worker nodes. The head node is responsible for managing the cluster, while the worker nodes execute tasks assigned by the head node. This architecture allows us to scale our OCR system horizontally by adding more worker nodes as needed.</p>
        
            <p>Here are the steps to set up a Ray cluster for OCR tasks:</p>
            
            <ol>
                <li><strong>Install Ray:</strong> To get started, install Ray on all nodes in the cluster. You can install Ray using pip:
                    <pre><code>pip install ray</code></pre>
                </li>
                <li><strong>Start the Head Node:</strong> On the head node, start the Ray cluster by running the following command:
                    <pre><code>ray start --head --port=6379</code></pre>
                    This command initializes the head node and opens port 6379 for communication with worker nodes.
                </li>
                <li><strong>Start the Worker Nodes:</strong> On each worker node, connect to the head node by running the following command:
                    <pre><code>ray start --address='head-node-ip:6379'</code></pre>
                    Replace <code>head-node-ip</code> with the IP address of the head node.
                </li>
                <li><strong>Initialize Ray in Your Python Script:</strong> In your Python script, initialize Ray by specifying the address of the head node:
                    <pre><code>import ray
        ray.init(address='auto')</code></pre>
                    This command connects your script to the Ray cluster, allowing you to distribute tasks across the cluster.
                </li>
            </ol>
        
            <h2>Distributed OCR Processing</h2>
            <p>With Ray set up, we can distribute OCR tasks across the cluster. Ray's actor-based concurrency model allows us to parallelize OCR processing, significantly improving the speed and efficiency of text extraction.</p>
        
            <p>In a distributed OCR system, each worker node is responsible for processing a subset of the data. For example, when processing a batch of images, each image can be assigned to a different worker node. The nodes process the images concurrently, extracting text in parallel.</p>
        
            <p>Here is an example of how to distribute OCR tasks using Ray:</p>
            
            <pre><code>import ray
        from easyocr import Reader
        
        # Initialize Ray
        ray.init(address='auto')
        
        @ray.remote
        class OCRWorker:
            def __init__(self):
                self.reader = Reader(['en'])
        
            def process_image(self, image_path):
                result = self.reader.readtext(image_path)
                return result
        
        # Create multiple OCRWorker actors
        workers = [OCRWorker.remote() for _ in range(4)]
        
        # Distribute OCR tasks across the workers
        image_paths = ['image1.jpg', 'image2.jpg', 'image3.jpg', 'image4.jpg']
        futures = [worker.process_image.remote(image) for worker, image in zip(workers, image_paths)]
        
        # Gather the results
        results = ray.get(futures)
        print(results)</code></pre>
        
            <p>In this example, we define an <code>OCRWorker</code> class that encapsulates the OCR logic using EasyOCR. We then create multiple instances of this class, each running on a different worker node. The <code>process_image</code> method processes a single image and returns the extracted text.</p>
        
            <p>We distribute the OCR tasks by assigning each image to a different worker node and use Ray's <code>remote</code> method to execute the tasks concurrently. Finally, we gather the results using <code>ray.get</code>, which blocks until all tasks are complete and returns the results.</p>
        
            <p>This approach allows us to scale our OCR system horizontally, processing multiple images in parallel and significantly reducing the time required for text extraction. By leveraging the distributed computing capabilities of Ray, we can build a robust and scalable OCR system that meets the demands of large-scale text recognition tasks.</p>
        </div>

        
        <div class="box">
    <h1>Optimizing Embedding with LangChain and Ray</h1>
    <p>Embedding text efficiently is a crucial component in modern Natural Language Processing (NLP) tasks. By leveraging LangChain and Ray, we can optimize this process to achieve significant performance improvements, making our embedding tasks up to 20 times faster.</p>

    <p>LangChain is a powerful framework designed for building applications with large language models. Combined with Ray, a high-performance distributed computing library, we can distribute embedding tasks across multiple nodes, thereby accelerating the embedding process.</p>

    <h2>Setting Up the Environment</h2>
    <p>To get started, ensure you have both LangChain and Ray installed in your environment. You can install these libraries using pip:</p>
    
    <pre><code>pip install langchain ray</code></pre>

    <p>With LangChain and Ray installed, we can proceed to set up the embedding workflow. Here's how you can do it:</p>

    <h2>Distributed Embedding Workflow</h2>
    <p>The first step is to initialize Ray and set up LangChain for embedding. We will create a Ray cluster with one head node and multiple worker nodes, similar to the setup used in OCR tasks. Then, we will use LangChain to handle the text embeddings.</p>

    <pre><code>import ray
from langchain.embeddings import Embeddings

# Initialize Ray
ray.init(address='auto')

@ray.remote
class EmbeddingWorker:
    def __init__(self):
        self.embedding_model = Embeddings(model_name="nomic-1.5v")

    def embed_text(self, text):
        embedding = self.embedding_model.embed(text)
        return embedding

# Create multiple EmbeddingWorker actors
workers = [EmbeddingWorker.remote() for _ in range(4)]

# Text data to be embedded
texts = [
    "The quick brown fox jumps over the lazy dog.",
    "LangChain and Ray can turbocharge embedding tasks.",
    "Distributed computing makes NLP tasks faster.",
    "Optimize your embedding workflow with Ray and LangChain."
]

# Distribute embedding tasks across the workers
futures = [worker.embed_text.remote(text) for worker, text in zip(workers, texts)]

# Gather the results
embeddings = ray.get(futures)
print(embeddings)</code></pre>

    <p>In this example, we define an <code>EmbeddingWorker</code> class that encapsulates the embedding logic using LangChain. Each instance of this class runs on a different worker node. The <code>embed_text</code> method takes a text input and returns its embedding.</p>

    <p>We create multiple instances of <code>EmbeddingWorker</code> and distribute the embedding tasks by assigning each text to a different worker node. The <code>remote</code> method is used to execute the tasks concurrently, and we gather the results using <code>ray.get</code>, which blocks until all tasks are complete and returns the embeddings.</p>

    <h2>Performance Optimization</h2>
    <p>To achieve optimal performance, consider the following best practices:</p>
    
    <ul>
        <li><strong>Load Balancing:</strong> Ensure the tasks are evenly distributed across the worker nodes to prevent any single node from becoming a bottleneck.</li>
        <li><strong>Resource Allocation:</strong> Adjust the resources allocated to each Ray actor based on the available hardware to maximize utilization.</li>
        <li><strong>Batch Processing:</strong> When dealing with large volumes of text, consider batching the text inputs to reduce the overhead associated with task initialization and communication.</li>
        <li><strong>Monitoring and Profiling:</strong> Use Ray's built-in tools for monitoring and profiling to identify and address performance bottlenecks.</li>
    </ul>

    <h2>Mathematical Foundations of Distributed Computing</h2>
    <p>Distributed computing leverages multiple processors to solve problems more efficiently than a single processor could. Mathematically, this involves dividing a problem into subproblems, distributing these subproblems among processors, and then combining the results.</p>

    <h3>Parallel Speedup</h3>
    <p>Let's define <code>T(1)</code> as the time it takes to execute a task on a single processor and <code>T(p)</code> as the time it takes to execute the task on <code>p</code> processors. The speedup <code>S(p)</code> achieved by using <code>p</code> processors is given by:</p>
    
    <p style="text-align:center;">\( S(p) = \frac{T(1)}{T(p)} \)</p>

    <p>Ideally, <code>S(p) = p</code>, indicating perfect linear speedup. However, due to overhead and communication costs, the actual speedup is usually less than <code>p</code>.</p>

    <h3>Amdahl's Law</h3>
    <p>Amdahl's Law provides a formula to find the maximum improvement to an overall system when only part of the system is improved. If <code>F</code> is the fraction of the task that can be parallelized, the maximum speedup <code>S_{\infty}</code> is:</p>
    
    <p style="text-align:center;">\( S_{\infty} = \frac{1}{(1 - F)} \)</p>

    <p>This indicates that even with an infinite number of processors, the speedup is limited by the sequential portion of the task.</p>

    <h2>Vector Databases and Embeddings</h2>
    <p>Vector databases store and index vectors (embeddings) for efficient similarity search. Embeddings are dense vector representations of data, often generated by deep learning models. These vectors capture the semantic meaning of the data.</p>

    <h3>Vector Representation</h3>
    <p>Consider a text corpus where each document <code>d</code> is represented as a vector <code>v_d</code> in a high-dimensional space. Mathematically, if <code>d</code> is a document and <code>E</code> is an embedding function, then:</p>
    
    <p style="text-align:center;">\( v_d = E(d) \)</p>

    <p>where <code>v_d \in \mathbb{R}^n</code> and <code>n</code> is the dimensionality of the embedding space.</p>

    <h3>Similarity Search</h3>
    <p>The goal of similarity search is to find documents whose embeddings are close to a query embedding <code>v_q</code>. The closeness is typically measured using a distance metric such as cosine similarity or Euclidean distance.</p>
    
    <p style="text-align:center;">\( \text{cosine\_similarity}(v_a, v_b) = \frac{v_a \cdot v_b}{\|v_a\| \|v_b\|} \)</p>
    
    <p style="text-align:center;">\( \text{Euclidean\_distance}(v_a, v_b) = \|v_a - v_b\|_2 \)</p>

    <p>Efficient similarity search in high-dimensional spaces is achieved using vector databases, which index embeddings and provide fast query capabilities.</p>

    <h2>Code for Efficient Embedding Search</h2>
    <p>Hereâ€™s an example of setting up a vector database and performing efficient embedding search:</p>

    <pre><code>from qdrant_client import QdrantClient
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Initialize Qdrant client
client = QdrantClient()

# Sample embeddings (usually generated by a model)
embeddings = np.random.rand(100, 128)  # 100 documents with 128-dimensional embeddings

# Add embeddings to the vector database
client.upload_collection(
    collection_name="embeddings_collection",
    vectors=embeddings,
    payload=None
)

# Query embedding
query_embedding = np.random.rand(1, 128)

# Retrieve the top-5 most similar embeddings
response = client.search(
    collection_name="embeddings_collection",
    query_vector=query_embedding,
    top=5
)

# Print the results
for hit in response['result']:
    print(f"Document ID: {hit['id']}, Similarity Score: {hit['score']}")
</code></pre>

    <p>In this code, we initialize a Qdrant client and upload a collection of sample embeddings. We then perform a search to find the top-5 most similar embeddings to a query embedding. The similarity scores indicate how close the documents are to the query.</p>

    <p>By combining LangChain for generating embeddings, Ray for distributed processing, and vector databases for efficient retrieval, we can build highly scalable and efficient NLP pipelines capable of handling large-scale embedding and search tasks.</p>
        </div>

        
               <div class="box">
            <h1>Creating a customized indexing and Quantization Layer with HNSW-FINGER and IVPFQ</h1>
            
            <h2>Hierarchical Navigable Small World (HNSW)</h2>
            <p>Hierarchical Navigable Small World (HNSW) is a state-of-the-art algorithm for approximate nearest neighbor search. It constructs a hierarchical graph where each layer is a proximity graph (small world) of the dataset. The hierarchical structure allows efficient navigation through the data points.</p>
            
            <p>The HNSW algorithm builds a multi-layer graph where the top layer contains a small subset of nodes and each lower layer has more nodes, finally including all data points at the bottom layer. Searching for nearest neighbors involves starting at the top layer and navigating down to the bottom layer, using the connections in each layer to quickly find close neighbors.</p>
            
            <p>The core idea of HNSW can be mathematically represented as:</p>
            
            <p>Let \( \mathcal{G} = (V, E) \) be a graph where \( V \) is the set of vertices (data points) and \( E \) is the set of edges (connections between points).</p>
            
            <p>At each layer \( l \) in the hierarchy, the graph \( \mathcal{G}_l = (V_l, E_l) \) is constructed such that:</p>
            
            <p>\[
            V_0 \subseteq V_1 \subseteq \cdots \subseteq V_L = V
            \]</p>
            
            <p>where \( L \) is the number of layers. The search algorithm starts at the top layer and moves down to the bottom layer:</p>
            
            <p>\[
            \text{Search}(q) = \text{Navigate}(\mathcal{G}_0, q) \rightarrow \text{Navigate}(\mathcal{G}_1, q) \rightarrow \cdots \rightarrow \text{Navigate}(\mathcal{G}_L, q)
            \]</p>
            
            <pre><code>import hnswlib
        
        # Initializing the HNSW index
        dim = 128  # Dimensionality of the embeddings
        num_elements = 10000  # Number of elements (data points)
        
        p = hnswlib.Index(space='l2', dim=dim)  # 'l2' refers to Euclidean distance
        p.init_index(max_elements=num_elements, ef_construction=200, M=16)
        
        # Generating random data points
        data = np.random.rand(num_elements, dim).astype(np.float32)
        
        # Adding data points to the index
        p.add_items(data)
        
        # Querying the index
        query_data = np.random.rand(1, dim).astype(np.float32)
        labels, distances = p.knn_query(query_data, k=5)  # Finding 5 nearest neighbors
        
        print("Nearest neighbors:", labels)
        print("Distances:", distances)</code></pre>
            
            <p>In this code snippet, we initialize an HNSW index using the `hnswlib` library, add random data points to the index, and query the index to find the nearest neighbors of a given query point.</p>
            
            <h2>Inverted File System with Product Quantization (IVF-PQ)</h2>
            <p>The Inverted File System with Product Quantization (IVF-PQ) is a technique for compressing high-dimensional vectors to reduce storage requirements and speed up similarity search. It divides the vector space into a number of subspaces and applies quantization within each subspace.</p>
            
            <p>Product Quantization (PQ) works by splitting the original vector into smaller sub-vectors and quantizing each sub-vector independently. This allows for efficient storage and computation of distances between compressed vectors.</p>
            
            <p>Mathematically, PQ can be described as follows:</p>
            
            <p>Given a vector \( \mathbf{x} \in \mathbb{R}^d \), we split it into \( m \) sub-vectors:</p>
            
            <p>\[
            \mathbf{x} = [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_m]
            \]</p>
            
            <p>Each sub-vector \( \mathbf{x}_i \) is quantized independently using a codebook \( \mathcal{C}_i \):</p>
            
            <p>\[
            \mathbf{q}_i = \text{argmin}_{\mathbf{c} \in \mathcal{C}_i} \| \mathbf{x}_i - \mathbf{c} \|
            \]</p>
            
            <p>The compressed vector \( \mathbf{q} \) is the concatenation of the quantized sub-vectors:</p>
            
            <p>\[
            \mathbf{q} = [\mathbf{q}_1, \mathbf{q}_2, \ldots, \mathbf{q}_m]
            \]</p>
            
            <pre><code>from faiss import IndexIVFPQ, IndexFlatL2
        
        # Parameters for IVF-PQ
        nlist = 100  # Number of clusters
        m = 8  # Number of sub-quantizers
        nbits = 8  # Number of bits per sub-vector
        
        # Initialize the IVF-PQ index
        quantizer = IndexFlatL2(dim)  # Quantizer for clustering
        index = IndexIVFPQ(quantizer, dim, nlist, m, nbits)
        
        # Training the index
        index.train(data)
        
        # Adding data points to the index
        index.add(data)
        
        # Querying the index
        D, I = index.search(query_data, k=5)  # Finding 5 nearest neighbors
        
        print("Nearest neighbors:", I)
        print("Distances:", D)</code></pre>
            
            <p>In this code snippet, we initialize an IVF-PQ index using the `faiss` library, train the index on the data points, and query the index to find the nearest neighbors of a given query point.</p>
            
            <h2>Product Quantization (PQ)</h2>
            <p>Product Quantization is a technique used to compress high-dimensional vectors by splitting them into smaller sub-vectors and quantizing each sub-vector independently. This results in a compact representation of the original vectors, enabling efficient storage and fast similarity search.</p>
            
            <p>PQ divides the original vector space into subspaces and applies vector quantization within each subspace. The quantized sub-vectors are then combined to form the final compressed vector. The distance between two compressed vectors is computed as the sum of the distances between their corresponding sub-vectors.</p>
            
            <p>Mathematically, the distance between two vectors \( \mathbf{x} \) and \( \mathbf{y} \) can be approximated as:</p>
            
            <p>\[
            \| \mathbf{x} - \mathbf{y} \|^2 \approx \sum_{i=1}^{m} \| \mathbf{x}_i - \mathbf{y}_i \|^2
            \]</p>
            
            <pre><code>from faiss import IndexPQ
        
        # Parameters for PQ
        m = 8  # Number of sub-quantizers
        nbits = 8  # Number of bits per sub-vector
        
        # Initialize the PQ index
        pq_index = IndexPQ(dim, m, nbits)
        
        # Training the index
        pq_index.train(data)
        
        # Adding data points to the index
        pq_index.add(data)
        
        # Querying the index
        D, I = pq_index.search(query_data, k=5)  # Finding 5 nearest neighbors
        
        print("Nearest neighbors:", I)
        print("Distances:", D)</code></pre>
            
            <p>In this code snippet, we initialize a PQ index using the `faiss` library, train the index on the data points, and query the index to find the nearest neighbors of a given query point.</p>
            
            <h2>HNSW-FINGER</h2>
            <p>HNSW-FINGER is an enhanced version of HNSW that incorporates fast inference techniques for graph-based approximate nearest neighbor search. It uses additional optimizations such as caching and pruning to further speed up the search process.</p>
            
            <p>The HNSW-FINGER algorithm builds on the HNSW approach by introducing a fingerprinting mechanism that caches intermediate results during the search process. This allows for faster inference by reusing previously computed distances and avoiding redundant computations.</p>
            
            <p>Let \( f(q, p) \) represent the fingerprint of the query \( q \) and point \( p \). The fingerprinting mechanism can be described as:</p>
            
            <p>\[
            f(q, p) = \begin{cases}
            d(q, p) & \text{if } d(q, p) \text{ is computed for the first time} \\
            \text{cached value} & \text{otherwise}
            \end{cases}
            \]</p>
            
            <pre><code># Assuming hnswlib is already imported and data is available
        
        # Initializing the HNSW-FINGER index (hypothetical example)
        class HNSWFinger(hnswlib.Index):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.cache = {}
        
            def knn_query(self, *args, **kwargs):
                # Implementing the caching mechanism
                if args in self.cache:
                    return self.cache[args]
                result = super().knn_query(*args, **kwargs)
                self.cache[args] = result
                return result
        
        # Create an instance of HNSW-FINGER
        hnsw_finger = HNSWFinger(space='l2', dim=dim)
        hnsw_finger.init_index(max_elements=num_elements, ef_construction=200, M=16)
        hnsw_finger.add_items(data)
        
        # Querying the index
        labels, distances = hnsw_finger.knn_query(query_data, k=5)
        
        print("Nearest neighbors:", labels)
        print("Distances:", distances)</code></pre>
            
            <p>In this hypothetical example, we extend the HNSW class to implement the HNSW-FINGER algorithm with caching. The `knn_query` method is overridden to include a caching mechanism that stores and retrieves previously computed results.</p>
            
            <h2>Combining HNSW-FINGER with IVF-PQ and Product Quantization</h2>
            <p>By combining the strengths of HNSW-FINGER, IVF-PQ, and Product Quantization, we can achieve even faster and more efficient similarity search. HNSW-FINGER provides a robust hierarchical graph-based approach for navigating the data points, while IVF-PQ and Product Quantization offer efficient storage and computation of distances between compressed vectors.</p>
            
            <p>Let's consider the combination mathematically. Given two vectors \( \mathbf{x} \) and \( \mathbf{y} \), the distance computation can be broken down as:</p>
            
            <p>1. **Hierarchical Search with HNSW-FINGER:**</p>
            
            <p>\[
            \text{Search}(q) = \sum_{l=0}^{L} \text{Navigate}(\mathcal{G}_l, q)
            \]</p>
            
            <p>2. **Quantization with IVF-PQ and PQ:**</p>
            
            <p>\[
            \| \mathbf{x} - \mathbf{y} \|^2 \approx \sum_{i=1}^{m} \| \mathbf{x}_i - \mathbf{y}_i \|^2
            \]</p>
            
            <pre><code>from faiss import IndexIVFPQ
        import hnswlib
        
        # Initialize the HNSW-FINGER index
        hnsw_finger = HNSWFinger(space='l2', dim=dim)
        hnsw_finger.init_index(max_elements=num_elements, ef_construction=200, M=16)
        hnsw_finger.add_items(data)
        
        # Initialize the IVF-PQ index
        quantizer = hnsw_finger  # Using HNSW-FINGER as the quantizer
        ivfpq_index = IndexIVFPQ(quantizer, dim, nlist, m, nbits)
        
        # Training the IVF-PQ index
        ivfpq_index.train(data)
        
        # Adding data points to the IVF-PQ index
        ivfpq_index.add(data)
        
        # Querying the index
        D, I = ivfpq_index.search(query_data, k=5)  # Finding 5 nearest neighbors
        
        print("Nearest neighbors:", I)
        print("Distances:", D)</code></pre>
            
            <p>In this code snippet, we initialize an HNSW-FINGER index and use it as the quantizer for an IVF-PQ index. We then train the IVF-PQ index on the data points and query the index to find the nearest neighbors of a given query point. This combination leverages the fast inference capabilities of HNSW-FINGER and the efficient storage and retrieval of IVF-PQ, resulting in a highly optimized similarity search pipeline.</p>
        </div>

        
        <div class="box">
            <h3>Retrieval Mechanism</h3>
            <p>The retrieval mechanism involves utilizing Qdrant and ChatWindowSummaryBuffer for efficient vector search and continuous chat summarization. Qdrant, a high-performance vector database, is employed to store and query the embeddings. The search operation involves querying the database with an embedding vector and retrieving the most similar vectors based on a specified distance metric.</p>
            <p>ChatWindowSummaryBuffer is used to manage long-term conversation memory, enabling continuous summarization of chat windows. This component helps in maintaining context and generating concise summaries of ongoing conversations, enhancing the user experience.</p>
        </div>
        
        <div class="box">
            <h3>AWS Cluster for File Ingestion and Embedding</h3>
            <p>For large-scale file ingestion and embedding, we set up an AWS cluster. The cluster is configured to ingest 1000 files within 1 minute and embed them within 3-4 minutes. This involves deploying a Ray cluster on AWS, specifying the number of worker nodes, and initializing Ray Serve on the cluster. The custom GROQ LLM server is integrated to handle the language model computations, ensuring efficient processing of large volumes of text data.</p>
        </div>
    </div>
</body>
</html>
