<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Text-SQL System: Optimizing Data for Vector Search</title>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: "JetBrains Mono", monospace;
            margin: 0;
            background-color: #121212;
            color: #ffffff;
        }

        nav {
            background-color: #1c1c1c;
            padding: 1rem;
            text-align: left;
        }

        nav a {
            font-family: "JetBrains Mono", monospace;
            color: #ffffff;
            margin: 0 1rem;
            text-decoration: none;
            font-weight: bold;
            transition: color 0.3s;
        }

        nav a:hover {
            color: #f39c12;
        }

        .content {
            padding: 2rem;
            max-width: 1200px;
            margin: auto;
        }

        .box {
            background-color: #2e2e2e;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            padding: 2rem;
            margin: 2rem 0;
            transition: transform 0.5s, box-shadow 0.5s;
        }

        .box img {
            max-width: 100%;
            border-radius: 8px;
        }

        h1, h2, h3 {
            color: #ffffff;
        }

        p, ul, ol {
            color: #b0b0b0;
        }

        .equation {
            color: #b0b0b0;
            text-align: center;
            margin: 1rem 0;
        }

        pre {
            background-color: #1e1e1e;
            border-radius: 4px;
            padding: 1rem;
            overflow-x: auto;
        }

        code {
            font-family: "JetBrains Mono", monospace;
            color: #e6e6e6;
        }

        .hypothesis, .test {
            border-left: 4px solid #f39c12;
            padding-left: 1rem;
            margin: 1rem 0;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="../index.html#home">Home</a>
        <a href="../index.html#projects">Projects</a>
        <a href="../index.html#publications">Publications</a>
        <a href="../hobbies.html">Hobbies</a>
    </nav>
    <div class="content">
        <h1>Text-SQL System: Optimizing Data for Vector Search</h1>

        <div class="box" id="introduction">
            <h2>1. Introduction</h2>
            <p>Hey there! Today, I want to talk about how we can optimize our data for better vector search. We're dealing with a massive scale hereâ€”10 PostgreSQL databases, each with 10-12 tables, and each table having 300-450 columns. That's a lot of data!</p>
            <p>We'll go through setting up metadata dictionaries, passing embeddings to a vector database (FAISS) with HNSW indexing and flat product quantization, querying for the most similar columns, and pushing this into the information space of a language model (LLM) with the prompt and metadata embedding.</p>
        </div>

        <div class="box" id="scale">
            <h2>2. The Scale of the Data</h2>
            <p>So, let's break down the scale of the data we're dealing with. We have 10 PostgreSQL databases, and each database has around 10-12 tables. Each of these tables has a whopping 300-450 columns. That's a lot of columns to keep track of!</p>
            <p>To give you an idea, if we have 10 databases, each with 12 tables, and each table with 450 columns, we're looking at:</p>
            <p class="equation">
                \[ \text{Total Columns} = 10 \times 12 \times 450 = 54,000 \text{ columns} \]
            </p>
            <p>That's a lot of data to manage and search through efficiently.</p>
        </div>

        <div class="box" id="metadata">
            <h2>3. Setting Up Metadata Dictionaries</h2>
            <p>To make sense of all this data, we need to set up metadata dictionaries. These dictionaries will contain information about column names, table names, original names, data types, query examples, and row data examples.</p>
            <p>Here's an example of what a metadata dictionary might look like:</p>
            <pre><code>
{
  "table_name": "sales_data",
  "columns": [
    {
      "column_name": "product_id",
      "original_name": "product_id",
      "data_type": "integer",
      "query_example": "SELECT product_id FROM sales_data WHERE product_id = 123;",
      "row_data_example": 123
    },
    {
      "column_name": "product_name",
      "original_name": "product_name",
      "data_type": "text",
      "query_example": "SELECT product_name FROM sales_data WHERE product_name LIKE '%chair%';",
      "row_data_example": "Ergonomic Office Chair"
    },
    // More columns...
  ]
}
            </code></pre>
            <p>This structure helps us keep track of all the important details about our data.</p>
        </div>

                  <div class="box" id="vector-database">
    <h2>4. Passing Embeddings to Vector Database (FAISS)</h2>
    <p>
        Next, we need to pass these embeddings to a vector database. We'll use FAISS (Facebook AI Similarity Search) with HNSW (Hierarchical Navigable Small World) indexing and flat product quantization.
    </p>
    <p>
        FAISS is a popular library for efficient similarity search and clustering of dense vectors. It provides various algorithms for both exact and approximate nearest neighbor searches, making it highly scalable and flexible.
    </p>
    
    <h3>HNSW Indexing</h3>
    <p>
        HNSW is a graph-based algorithm that allows for efficient similarity search in high-dimensional spaces. The algorithm works by constructing a multi-layered graph, where each node represents a data point, and edges connect similar nodes. This hierarchical structure allows for fast nearest neighbor searches by navigating through different layers of the graph.
    </p>
    <p>
        Mathematically, the HNSW algorithm can be represented as:
    </p>
    <p class="equation">
        \( \text{HNSW}(G, q) = \arg\min_{v \in G} d(q, v) \)
    </p>
    <p>
        Where \( G \) is the graph, \( q \) is the query vector, and \( d \) is the distance metric (e.g., Euclidean distance).
    </p>
    <p>
        The process of building the HNSW graph can be understood as follows:
    </p>
    <ol>
        <li><strong>Initialization:</strong> Start by creating the first layer of the graph with all data points. Each node in this layer is connected to a certain number of nearest neighbors.</li>
        <li><strong>Layer Creation:</strong> Additional layers are created by selecting a subset of nodes from the previous layer. These layers become progressively sparser.</li>
        <li><strong>Graph Navigation:</strong> During the search, the algorithm starts from the topmost layer and navigates downwards, moving from one node to its nearest neighbors until the closest nodes in the lowest layer are found.</li>
    </ol>
    <p>
        This hierarchical approach significantly reduces the search complexity and ensures a balance between accuracy and speed. For a more detailed understanding, you can refer to the <a href="https://arxiv.org/pdf/1603.09320">HNSW paper</a>.
    </p>
    
    <h3>Flat Product Quantization</h3>
    <p>
        Flat product quantization is a dimensionality reduction technique that reduces the storage and computation requirements while preserving the similarity information of the vectors. This technique decomposes the original high-dimensional vector space into a product of lower-dimensional subspaces, which are quantized independently.
    </p>
    <p>
        The decomposition can be mathematically represented as:
    </p>
    <p class="equation">
        \( V = V_1 \times V_2 \times \ldots \times V_k \)
    </p>
    <p>
        Where \( V \) is the original vector space, and \( V_1, V_2, \ldots, V_k \) are the lower-dimensional subspaces.
    </p>
    <p>
        Each subspace \( V_i \) is quantized separately, and the product quantization (PQ) process can be illustrated as follows:
    </p>
    <ol>
        <li><strong>Partitioning:</strong> The original vector space \( V \) is partitioned into \( k \) subspaces, each of which is lower-dimensional.</li>
        <li><strong>Quantization:</strong> Each subspace \( V_i \) is quantized independently. The quantization can be achieved by selecting a set of centroids (codebook) that best represent the data points within each subspace.</li>
        <li><strong>Encoding:</strong> Each vector in the original space is represented as a concatenation of the indices of the nearest centroids in each subspace. This process results in a compressed representation of the vectors.</li>
    </ol>
    <p>
        The goal of product quantization is to minimize the quantization error, which is the difference between the original vector and its quantized representation. This process is crucial in scenarios where large-scale vector data needs to be stored and queried efficiently.
    </p>

    <h3>Quantization Rate by Product Quantizers</h3>
    <p>
        The optimal product quantizer can be found by solving the integral bit allocation problem:
    </p>
    <p class="equation">
        \[ \min \left\{ \|W - \hat{W}(N_1, \dots, N_m, \text{prod})\|^2, N_1, \dots, N_m \geq 1, N_1 \times \dots \times N_m \leq N, m \geq 1 \right\} \]
    </p>
    <p>
        Expanding this, we get:
    </p>
    <p class="equation">
        \[ \|W - \hat{W}(N_1, \dots, N_m, \text{prod})\|^2_2 = \sum_{n \geq 1} \lambda_n \|\hat{\xi}_n^{N_n} - \xi_n\|^2_2 \]
    </p>
    <p>
        This further simplifies to:
    </p>
    <p class="equation">
        \sum_{n=1}^m \lambda_n \left( e^{N_n^2(N(0;1), \mathbb{R})} - 1 \right) + T^2_2
    </p>
    <p>
        Given that:
    </p>
    <p class="equation">
        \sum_{n \geq 1} \lambda_n = E \sum_{n \geq 1} \left(W | e_n W\right)^2_2 = E \int_0^T W_t^2 dt = \int_0^T t dt = T^2_2.
    </p>
    <p>
        According to Theorem 6.3 (as referenced in Luschgy and PagÃ¨s [2002]), for every \( N \geq 1 \), there exists an optimal scalar product quantizer of size at most \( N \) (or at level \( N \)), denoted by \( \hat{W}(N, \text{prod}) \), of the Brownian motion defined as the solution to the minimization problem. These optimal product quantizers form a rate-optimal sequence, such that:
    </p>
    <p class="equation">
        \|W - \hat{W}(N, \text{prod})\|_2 \leq c_W T (\log N)^{1/2}.
    </p>
    <p>
        A sketch of the proof shows that by assuming \( T = 1 \) without loss of generality and combining the expansion with Zador's theorem, we get:
    </p>
    <p class="equation">
        \|W - \hat{W}(N_1, \dots, N_m, \text{prod})\|^2_2 \leq C \left( \sum_{n=1}^m \frac{1}{n^2 N_n^2} \right) + \sum_{n \geq m+1} \lambda_n \leq C' \left( \sum_{n=1}^m \frac{1}{n^2 N_n^2} + \frac{1}{m} \right),
    </p>
    <p>
        where \( \Pi_n N_n \leq N \). Setting \( m := m(N) = \lceil \log N \rceil \) and \( N_k = \lceil (m! N)^{1/m}/k \rceil \geq 1, \) for \( k = 1, \dots, m \), gives the upper bound as described.
    </p>
    <p>
        For a more comprehensive explanation of product quantization and the mathematical derivations, you can explore this <a href="https://www.sciencedirect.com/topics/computer-science/product-quantization">detailed resource</a>.
    </p>

    <h2>2.11 Quantization in Digital Filters</h2>
    <p>
        Quantization errors in digital filters can be classified as:
    </p>
    <ul>
        <li>Round-off errors derived from internal signals that are quantized before or after more down additions;</li>
        <li>Deviations in the filter response due to finite word length representation of multiplier coefficients; and</li>
        <li>Errors due to the representation of the input signal with a set of discrete levels.</li>
    </ul>
    <p>
        A general digital filter structure with quantizers before delay elements can be represented as shown below, with the quantizers implementing rounding for the granular quantization and saturation arithmetic for the overflow nonlinearity.
    </p>
    <img src="https://ars.els-cdn.com/content/image/3-s2.0-B9780121709600500621-f60-23-9780121709600.gif" alt="Digital Filter Including Quantizers at the Delay Inputs">
    <p class="image-caption">Digital Filter Including Quantizers at the Delay Inputs</p>
    
    <h3>2.11.1 Coefficient Quantization</h3>
    <p>
        Approximations are known to generate digital filter coefficients with high accuracy. After coefficient quantization, the frequency response of the realized digital filter will deviate from the ideal response and eventually fail to meet the prescribed specifications. The sensitivity of the filter response to coefficient quantization varies with the structure. The development of low-sensitivity digital filter realizations has garnered significant interest (Antoniou, 1993; Diniz et al., 2002).
    </p>
    <p>
        A common procedure is to design the digital filter with infinite coefficient word length, satisfying tighter specifications than required. Then, quantize the coefficients and check if the prescribed specifications are still met.
    </p>

    <h3>2.11.2 Quantization Noise</h3>
    <p>
        In fixed-point arithmetic, a number with a modulus less than one can be represented as follows:
    </p>
    <p class="equation">
        \( x = b_0b_1b_2b_3 \ldots b_b, \)
    </p>
    <p>
        where \( b_0 \) is the sign bit, and \( b_1b_2b_3 \ldots b_b \) represent the modulus using a binary code. For digital filtering, the most widely used binary code is the two's complement representation, where for positive numbers \( b_0 = 0 \) and for negative numbers \( b_0 = 1 \). The fractionary part of the number, called \( x_2 \), is represented as:
    </p>
    <p class="equation">
        \( x_2 = \begin{cases} 
        x & \text{if } b_0 = 0, \\
        2^{-|x|} & \text{if } b_0 = 1.
        \end{cases} \)
    </p>
    <p>
        The discussion here focuses on the fixed-point implementation.
    </p>
    <p>
        A finite word length multiplier can be modeled in terms of an ideal multiplier followed by a single noise source \( e(n) \) as shown below:
    </p>
    <img src="path_to_image_figure_2_24.png" alt="Model for the Noise Generated after a Multiplication">
    <p class="image-caption">FIGURE 2.24. Model for the Noise Generated after a Multiplication</p>
    <p>
        For product quantization performed by rounding and for signal levels throughout the filter much larger than the quantization step \( q = 2^{-b} \), it can be shown that the power spectral density of the noise source \( e_i(n) \) is given by:
    </p>
    <p class="equation">
        \( P_{e_i}(z) = \frac{q^2}{12} = \frac{2^{-2b}}{12}. \)
    </p>
    <p>
        In this case, \( e_i(n) \) represents a zero mean white noise process. In practice, \( e_i(n) \) and \( e_k(n + l) \) are statistically independent for any value of \( n \) or \( l \) (for \( i \neq k \)). As a result, the contributions of different noise sources can be considered separately using the principle of superposition.
    </p>
    <p>
        The power spectral density of the output noise in a fixed-point digital-filter implementation is given by:
    </p>
    <p class="equation">
        \( P_y(z) = \sigma_e^2 \sum_{i=1}^{K} G_i(z)G_i(z^{-1}), \)
    </p>
    <p>
        where \( P_{e_i}(e^{j\omega}) = \sigma_e^2 \) for all \( i \), and each \( G_i(z) \) is a transfer function from multiplier output \( g_i(n) \) to the output of the filter as shown in Figure 2.25. The word length, including the sign bit, is \( b + 1 \) bits, and \( K \) is the number of multipliers in the filter.
    </p>
    <img src="https://ars.els-cdn.com/content/image/3-s2.0-B9780121709600500621-f60-25-9780121709600.gif" alt="Digital Filter Including Scaling and Noise Transfer Functions">
    <p class="image-caption"> Digital Filter Including Scaling and Noise Transfer Functions.</p>

    <h3> Overflow Limit Cycles</h3>
    <p>
        Overflow nonlinearities influence the most significant bits of the signal and cause severe distortion. An overflow can give rise to self-sustained, high-amplitude oscillations known as overflow limit cycles. Digital filters that are free of zero-input limit cycles are also free of overflow oscillations if the overflow nonlinearities are implemented with saturation arithmetic, replacing the number in overflow with a number of the same sign and maximum magnitude that fits the available word length.
    </p>
    <p>
        When an input signal is applied to a digital filter, overflow might occur. As a result, input signal scaling is required to reduce the probability of overflow to an acceptable level. Ideally, signal scaling should be applied to ensure that the probability of overflow is the same at each internal node of the digital filter. This maximizes the signal-to-noise ratio in fixed-point implementations.
    </p>
    <p>
        In two's complement arithmetic, adding more than two numbers will be correct regardless of the order in which they are added, even if overflow occurs in partial summation, as long as the overall sum is within the available range. Therefore, a simplified scaling technique can be used where only the multiplier inputs require scaling. A multiplier is used at the input of the filter section to perform scaling, as illustrated in Figure 2.25.
    </p>
    <p>
        The signal at the multiplier input is given by:
    </p>
    <p class="equation">
        \( x_i(n) = \frac{1}{2\pi j} \oint_{c} X_i(z)z^{n-1} dz = \frac{1}{2\pi} \int_{0}^{2\pi} F_i(e^{j\omega})X(e^{j\omega})e^{j\omega n} d\omega, \)
    </p>
    <p>
        where \( c \) is the convergence region common to \( F_i(z) \) and \( X(z) \).
    </p>
    <p>
        The constant \( \lambda \) is usually calculated using the \( L_p \) norm of the transfer function from the filter input to the multiplier input \( F_i(z) \), depending on the known properties of the input signal. The \( L_p \) norm of \( F_i(z) \) is defined as:
    </p>
    <p class="equation">
        \( \|F_i(e^{j\omega})\|_p = \left[\frac{1}{2\pi} \int_{0}^{2\pi} |F_i(e^{j\omega})|^p d\omega\right]^{\frac{1}{p}}, \)
    </p>
    <p>
        for each \( p \geq 1 \), such that \( \int_{0}^{2\pi} |F_i(e^{j\omega})|^p d\omega \leq \infty \). In general, the following inequality holds:
    </p>
    <p class="equation">
        \( |x_i(n)| \leq \|F_i\|_p \|X\|_q, \quad \left(\frac{1}{p} + \frac{1}{q} = 1\right), \)
    </p>
    <p>
        for \( p, q = 1, 2, \) and \( \infty \).
    </p>
    <p>
        The scaling ensures that the magnitudes of multiplier inputs are bounded by a number \( M_{\text{max}} \) when \( |x(n)| \leq M_{\text{max}} \). To ensure that all multiplier inputs are bounded by \( M_{\text{max}} \), \( \lambda \) must be chosen as follows:
    </p>
    <p class="equation">
        \( \lambda = \frac{1}{\text{Max}\{\|F_1\|_p, \ldots, \|F_K\|_p\}}, \)
    </p>
    <p>
        which means that:
    </p>
    <p class="equation">
        \( \|F'_i(e^{j\omega})\|_p \leq 1, \text{ for } \|X(e^{j\omega})\|_q \leq M_{\text{max}}. \)
    </p>
    <p>
        The number \( K \) is the number of multipliers in the filter.
    </p>
    <p>
        The norm \( p \) is usually chosen to be infinity or 2. The \( L_{\infty} \) norm is used for input signals with a dominant frequency component, whereas the \( L_2 \) norm is more suitable for a random input signal. Scaling coefficients can be implemented by simple shift operations provided they satisfy the overflow constraints.
    </p>
    <p>
        For modular realizations, such as cascade or parallel realizations of digital filters, optimum scaling is accomplished by applying one scaling multiplier per section.
    </p>
    <p>
        As an illustration, the equation to compute the scaling factor for the cascade realization with direct-form second-order sections is presented as follows:
    </p>
    <p class="equation">
        \( \lambda_i = \frac{1}{\|\prod_{j=1}^{i-1} H_j(z) F_i(z)\|_p}, \)
    </p>
    <p>
        where:
    </p>
    <p class="equation">
        \( F_i(z) = \frac{1}{z^2 + m_{1i}z + m_{2i}}. \)
    </p>
    <p>
        The noise power spectral density is computed as:
    </p>
    <p class="equation">
        \( P_y(z) = \sigma_e^2 \left[3 + 3\lambda_1^2 \prod_{i=1}^{m} H_i(z)H_i(z^{-1}) + 5 \sum_{j=2}^{m} \lambda_j^2 \prod_{i=j}^{m} H_i(z)H_i(z^{-1}) \right], \)
    </p>
    <p>
        whereas the output noise variance is given by:
    </p>
    <p class="equation">
        \( \sigma_o^2 = \sigma_e^2 \left[3 + 3\lambda_1^2 \|\prod_{i=1}^{m} H_i(e^{j\omega})\|_2^2 + 5 \sum_{j=2}^{m} \lambda_j^2 \|\prod_{i=j}^{m} H_i(e^{j\omega})\|_2^2 \right]. \)
    </p>
    <p>
        As a design rule, the pairing of poles and zeros is performed as follows: poles closer to the unit circle pair with closer zeros to themselves, minimizing \( \|H_i(z)\|_p \) for \( p = 2 \) or \( p = \infty \).
    </p>
    <p>
        For ordering, we define the following:
    </p>
    <p class="equation">
        \( P_i = \frac{\|H_i(z)\|_{\infty}}{\|H_i(z)\|_2}. \)
    </p>
    <p>
        For \( L_2 \) scaling, we order the section such that \( P_i \) is decreasing. For \( L_{\infty} \) scaling, \( P_i \) should be increasing.
    </p>

    <h3>Granularity Limit Cycles</h3>
    <p>
        Granularity limit cycles occur in digital filters when the quantization step size is too large, leading to a situation where the output oscillates between a small number of discrete values. This is typically seen in recursive digital filters where the feedback loop amplifies the quantization error. Granularity limit cycles can cause severe distortion in the output signal and are generally undesirable. They can be minimized by reducing the quantization step size or by using dithering techniques, which add a small amount of noise to the signal before quantization to decorrelate the quantization error from the signal.
    </p>
</div>


        <div class="box" id="querying">
            <h2>5. Querying the Vector Database</h2>
            <p>Once we have our embeddings in the vector database, we can query it for the top N most semantically and contextually similar columns. This is where the magic happens!</p>
            <p>We can use a similarity metric like cosine similarity to find the most similar columns. The cosine similarity between two vectors A and B is calculated as:</p>
            <p class="equation">
                \[ \text{similarity} = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2} \sqrt{\sum_{i=1}^n B_i^2}} \]
            </p>
            <p>The cosine distance is then defined as \(1 - \text{similarity}\).</p>
            <p>Another important metric is the Euclidean distance, which is calculated as:</p>
            <p class="equation">
                \[ d(A, B) = \sqrt{\sum_{i=1}^n (A_i - B_i)^2} \]
            </p>
            <p>For high-dimensional spaces, the Manhattan distance can also be useful:</p>
            <p class="equation">
                \[ d(A, B) = \sum_{i=1}^n |A_i - B_i| \]
            </p>
            <p>These distance metrics help us find the most similar columns in the vector space.</p>
        </div>

        <div class="box" id="llm">
            <h2>6. Pushing into the LLM Information Space</h2>
            <p>Finally, we push this information into the information space of a language model (LLM) with the prompt and the metadata embedding. This allows the LLM to understand the context and semantics of the data better.</p>
            <p>The LLM can then use this information to generate more accurate and relevant queries. For example, if we ask the LLM to find all products related to "office chairs," it can use the metadata and embeddings to find the most relevant columns and generate a query like:</p>
            <pre><code>
SELECT product_name, price FROM sales_data WHERE product_name LIKE '%office chair%';
            </code></pre>
            <p>The process of embedding the metadata can be represented as:</p>
            <p class="equation">
                \[ E = \text{Embedding}(M) \]
            </p>
            <p>Where \( E \) is the embedding vector, and \( M \) is the metadata dictionary. The embedding function can be a neural network or any other suitable model.</p>
            <p>The LLM then uses this embedding to generate the query:</p>
            <p class="equation">
                \[ Q = \text{LLM}(E, P) \]
            </p>
            <p>Where \( Q \) is the generated query, \( E \) is the embedding vector, and \( P \) is the prompt.</p>
        </div>

        <div class="box" id="conclusion">
            <h2>7. Conclusion</h2>
            <p>And that's it! By setting up metadata dictionaries, passing embeddings to a vector database, querying for the most similar columns, and pushing this information into the LLM, we can optimize our data for better vector search.</p>
            <p>This approach allows us to handle large-scale data efficiently and generate more accurate and relevant queries. It's a win-win!</p>
        </div>
    </div>
</body>
</html>
